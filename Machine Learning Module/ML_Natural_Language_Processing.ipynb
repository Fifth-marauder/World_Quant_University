{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%logstop\n",
    "%logstart -rtq ~/.logs/ML_Natural_Language_Processing.py append\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "Natural language processing (NLP) is the field devoted to methods and algorithms for processing human (natural) languages for computers. NLP is a vast discipline that is actively being researched. For this notebook, we will be concerned with NLP tools and techniques we can use for machine learning applications. Some examples of machine learning applications using NLP include sentiment analysis, topic modeling, and language translation. In NLP, the following terms have specific meanings:\n",
    "\n",
    "* **Corpus**: The body/collection of text being investigated.\n",
    "* **Document**: The unit of analysis, what is considered a single observation.\n",
    "\n",
    "Examples of corpora include a collection of reviews and tweets, the text of the _Iliad_, and Wikipedia articles. Documents can be whatever you decided, it is what your model will consider an observation. For the example when the corpus is a collection of reviews or tweets, it is logical to make the document a single review or tweet. For the example of the text of the _Iliad_, we can set the document size to a sentence or a paragraph. The choice of document size will be influenced by the size of our corpus. If it is large, it may make sense to call each paragraph a document. As is usually the case, some design choices that need to be made.\n",
    "\n",
    "For this notebook, we will build a classifier to discern homonyms, words that are spelled the same but that have different meanings. The exact use case we will explore is to discern if the word \"python\" refers to the programming language or the animal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP with spaCy\n",
    "\n",
    "spaCy is a Python package that bills itself as \"industrial-strength\" natural language processing. We will use the tools spaCy provides in conjunction with `scikit-learn`. Let's explore some of spaCy's capabilities; we will introduce more functionality when needed. More about spaCy can be found [here](https://spacy.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's try out spacy.\n",
      "We can easily divide our text into sentences!\n",
      "I've run out of ideas.\n",
      "Let\n",
      "'s\n",
      "We\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# load text processing pipeline\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# nlp accepts a string\n",
    "doc = nlp(\"Let's try out spacy. We can easily divide our text into sentences! I've run out of ideas.\")\n",
    "\n",
    "# iterate through each sentence\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "\n",
    "# index words\n",
    "print(doc[0])\n",
    "print(doc[1])\n",
    "print(doc[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another nice feature from spaCy is part-of-speech tagging, the process of identifying whether a word is a noun, adjective, adverb, etc. A processed word has the attribute `pos_` and `tag_`; the former identifies the simple part of speech (e.g., noun) wile the latter identifies the more detailed part of speech (e.g., proper noun). The meaning of the resulting abbreviations of the `tag_` are listed [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) or can be revealed by running `spacy.explain` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'DET', 'DT')\n",
      "('quick', 'ADJ', 'JJ')\n",
      "('brown', 'ADJ', 'JJ')\n",
      "('fox', 'PROPN', 'NNP')\n",
      "('jumped', 'VERB', 'VBD')\n",
      "('over', 'ADP', 'IN')\n",
      "('the', 'DET', 'DT')\n",
      "('lazy', 'ADJ', 'JJ')\n",
      "('dog', 'NOUN', 'NN')\n",
      "('.', 'PUNCT', '.')\n",
      "('Mr.', 'PROPN', 'NNP')\n",
      "('Peanut', 'PROPN', 'NNP')\n",
      "('wears', 'VERB', 'VBZ')\n",
      "('a', 'DET', 'DT')\n",
      "('top', 'ADJ', 'JJ')\n",
      "('hat', 'NOUN', 'NN')\n",
      "('.', 'PUNCT', '.')\n",
      "\n",
      ". punctuation mark, sentence closer\n",
      "NNP noun, proper singular\n",
      "NN noun, singular or mass\n",
      "VBZ verb, 3rd person singular present\n",
      "DT determiner\n",
      "VBD verb, past tense\n",
      "IN conjunction, subordinating or preposition\n",
      "JJ adjective\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"The quick brown fox jumped over the lazy dog. Mr. Peanut wears a top hat.\")\n",
    "tags = set()\n",
    "\n",
    "# reveal part of speech\n",
    "for word in doc:\n",
    "    tags.add(word.tag_)\n",
    "    print((word.text, word.pos_, word.tag_))\n",
    "\n",
    "# revealing meaning of tags\n",
    "print()\n",
    "for tag in tags:\n",
    "    print(tag, spacy.explain(tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining a corpus\n",
    "\n",
    "Before we can move on with our analysis, we need to obtain a corpus. For our intended classifier, we need documents pertaining to python the animal and Python the programming language. Let's use Wikipedia articles to form our corpus. Luckily, there's a Python package called `wikipedia` that makes it easy to fetch articles. We will create documents based on the sentences in the articles. The function allows us to pass multiples pages in constructing the documents, allowing us to prevent one class of documents from dominating the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python is an interpreted high-level general-purpose programming language.', \"Python's design philosophy emphasizes code readability with its notable use of significant indentation.\", 'Its language constructs as well as its object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.', 'Python is dynamically-typed and garbage-collected.', 'It supports multiple programming paradigms, including structured (particularly, procedural), object-oriented and functional programming.']\n",
      "\n",
      "['The reticulated python (Malayopython reticulatus) is a python species native to South and Southeast Asia.', \"It is the world's longest snake, and listed as least concern on the IUCN Red List because of its wide distribution.\", 'In several countries in its range, it is hunted for its skin, for use in traditional medicine, and for sale as a pet.', 'It is an excellent swimmer, has been reported far out at sea, and has colonized many small islands within its range.\\n', 'It is among the three heaviest snakes.']\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "\n",
    "def pages_to_sentences(*pages):\n",
    "    \"\"\"Return a list of sentences in Wikipedia articles.\"\"\"\n",
    "    sentences = []\n",
    "    \n",
    "    for page in pages:\n",
    "        p = wikipedia.page(page)\n",
    "        doc = nlp(p.content)\n",
    "        sentences += [sent.text for sent in doc.sents]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "animal_sents = pages_to_sentences(\"Reticulated python\", \"Ball Python\")\n",
    "language_sents = pages_to_sentences(\"Python (programming language)\")\n",
    "documents = animal_sents + language_sents\n",
    "\n",
    "print(language_sents[:5])\n",
    "print()\n",
    "print(animal_sents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "* Given the example documents, what patterns should our word usage classifier learn?\n",
    "* We chose to create documents from sentences. What are other options? What are some pros and cons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The reticulated python (Malayopython reticulatus) is a python species native to South and Southeast Asia.',\n",
       " \"It is the world's longest snake, and listed as least concern on the IUCN Red List because of its wide distribution.\",\n",
       " 'In several countries in its range, it is hunted for its skin, for use in traditional medicine, and for sale as a pet.',\n",
       " 'It is an excellent swimmer, has been reported far out at sea, and has colonized many small islands within its range.\\n',\n",
       " 'It is among the three heaviest snakes.',\n",
       " 'Like all pythons, it is a non-venomous constrictor.',\n",
       " 'Adult humans have been killed (and in at least two reported cases, eaten) by reticulated pythons.\\n\\n\\n',\n",
       " '=',\n",
       " '=',\n",
       " 'Taxonomy ==\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "858"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words model\n",
    "\n",
    "Machine learning models needs to ingest data in a structured form, a matrix where the rows represents observations and the columns are features/attributes. When working with text data, we need a method to convert this unstructured data into a form that the machine learning model can work with. Let's consider our motivating example to create a classifier to discern the usage of \"python\" in a document. We understand that documents referring to the programming language will use words such as \"integer\", \"byte\", and \"error\" at higher frequency than documents that refer to python the animal. The reverse is true for words such as \"bite\", \"snake\", and \"pet\". One technique to _transform_ text data into a matrix is to count the number of appearances of each word in each document. This technique is called the **bag of words** model. The model gets its name because each document is viewed as a bag holding all the words, disregarding word order, context, and grammar. After applying the bag of words model to a corpus, the resulting matrix will exhibit patterns that a machine learning model can exploit. See the example below for the result of applying the bag of words model to a corpus of two documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document 0: \"The python is a large snake, although the snake is not venomous.\" <br>\n",
    "Document 1: \"Python is an interpreted programming language for general purpose programming.\" <br>\n",
    "<br>\n",
    "\n",
    "| although | an | for | general | interpreted | is | language | large | not | programming | purpose | python | snake | the | venomous |\n",
    "|:--------:|----|-----|---------|-------------|----|----------|-------|-----|-------------|---------|--------|-------|-----|----------|\n",
    "|     1    | 0  | 0   | 0       | 0           | 2  | 0        | 1     | 1   | 0           | 0       | 1      | 2     | 2   | 1        |\n",
    "|     0    | 1  | 1   | 1       | 1           | 1  | 1        | 0     | 0   | 2           | 1       | 1      | 0     | 0   | 0        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `CountVectorizer` transformer\n",
    "\n",
    "The bag of words model is found in `scikit-learn` with the `CountVectorizer` transformer. Note, `scikit-learn` uses the word `Vectorizer` to refer to transformers that convert a data structure (like a dictionary) into a NumPy array. Since it is a transformer, we need to first fit the object and _then_ call `transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 235)\t1\n",
      "  (0, 286)\t1\n",
      "  (0, 1407)\t1\n",
      "  (0, 1595)\t1\n",
      "  (0, 1759)\t1\n",
      "  (0, 2096)\t2\n",
      "  (0, 2228)\t1\n",
      "  (0, 2230)\t1\n",
      "  (0, 2459)\t1\n",
      "  (0, 2460)\t1\n",
      "  (0, 2472)\t1\n",
      "  (0, 2648)\t1\n",
      "  (0, 2686)\t1\n",
      "  (1, 235)\t1\n",
      "  (1, 285)\t1\n",
      "  (1, 351)\t1\n",
      "  (1, 620)\t1\n",
      "  (1, 817)\t1\n",
      "  (1, 1407)\t1\n",
      "  (1, 1413)\t1\n",
      "  (1, 1425)\t1\n",
      "  (1, 1427)\t1\n",
      "  (1, 1507)\t1\n",
      "  (1, 1541)\t1\n",
      "  (1, 1542)\t1\n",
      "  :\t:\n",
      "  (846, 2096)\t1\n",
      "  (847, 261)\t1\n",
      "  (848, 69)\t1\n",
      "  (848, 97)\t1\n",
      "  (848, 138)\t1\n",
      "  (848, 1408)\t1\n",
      "  (849, 53)\t1\n",
      "  (849, 1620)\t1\n",
      "  (849, 2574)\t1\n",
      "  (850, 76)\t1\n",
      "  (850, 870)\t1\n",
      "  (850, 1312)\t1\n",
      "  (850, 2051)\t1\n",
      "  (850, 2096)\t1\n",
      "  (851, 172)\t1\n",
      "  (851, 2045)\t1\n",
      "  (851, 2854)\t1\n",
      "  (852, 138)\t1\n",
      "  (852, 1408)\t1\n",
      "  (853, 82)\t1\n",
      "  (853, 121)\t1\n",
      "  (856, 1002)\t1\n",
      "  (856, 1538)\t1\n",
      "  (857, 1842)\t1\n",
      "  (857, 2842)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<858x2934 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9729 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bag_of_words = CountVectorizer()\n",
    "bag_of_words.fit(documents)\n",
    "word_counts = bag_of_words.transform(documents)\n",
    "\n",
    "print(word_counts)\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVectorizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transform` method returns a sparse matrix. A sparse matrix is a more efficient manner of storing a matrix. If a matrix has mostly zero entries, it is better to just store the non-zero entries and their occurrence, their row and column. Sparse matrices have the method `toarray()` that returns a full matrix **but** doing so may result in memory issues. Some key hyperparameters of the `CountVectorizer` are shown below:\n",
    "\n",
    "* `min_df`: only counts words that appear in a minimum number of documents.\n",
    "* `max_df`: only counts words that do not appear more than a maximum number of documents.\n",
    "* `max_features`: limits the number of generated features, based on the frequency.\n",
    "\n",
    "After fitting a `CountVectorizer` object, the following method and attribute help with determining which index belongs to which word.\n",
    "\n",
    "* `get_feature_names()`: Returns a list of words used as features. The index of the word corresponds to the column index.\n",
    "* `vocabulary_`: A dictionary mapping a word to its corresponding feature index.\n",
    "\n",
    "Let's use `vocabulary_` to determine how many times \"programming\" occurs in the documents for Python the programming language and python the animal. Do the results make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'check'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.get_feature_names()[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.vocabulary_['checking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "# get word counts\n",
    "counts_animal = bag_of_words.transform(animal_sents)\n",
    "counts_language = bag_of_words.transform(language_sents)\n",
    "\n",
    "# index for \"programming\"\n",
    "ind_programming = bag_of_words.vocabulary_['programming']\n",
    "\n",
    "# total counts across all documents\n",
    "print(counts_animal.sum(axis=0)[0, ind_programming])\n",
    "print(counts_language.sum(axis=0)[0, ind_programming])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `HashingVectorizer` transformer\n",
    "\n",
    "The `CountVectorizer` requires that we hold the mapping of words to features in memory. In addition, document processing cannot be parallelized because each worker needs to have the same mapping of word to column index. `CountVectorizer` objects are said to have _state_, they retain information of previous interactions and usage. A trick to improve the `CountVectorizer` is to use a hash function to convert the words into numbers. A hash function is a function that converts an input into a _deterministic_ value. In our context, we will use a hash function to convert a word into a number. The resulting number determines which feature column the word is mapped to. Python has a built-in hash function, seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1061797125128676474\n",
      "-6918077042183725629\n",
      "-599794237859629814\n",
      "1061797125128676474\n"
     ]
    }
   ],
   "source": [
    "print(hash(\"hi!\"))\n",
    "print(hash(\"python\"))\n",
    "print(hash(\"Python\"))\n",
    "print(hash(\"hi!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the function returns different values for different words. Also notice, the hash values of \"apple\" and \"apples\" are significantly different. Ideally no two inputs result in the same hash value, but this is impossible to avoid; when different inputs generate the same hash, it is referred to as a \"hash collision\".\n",
    "\n",
    "The `HashingVectorizer` class is similar to the `CountVectorizer` but it uses a hash function to render it *stateless*. The stateless nature of `HashingVectorizer` objects allows it to parallelize the counting process. There are two main disadvantages of `HashingVectorizer`:\n",
    "\n",
    "* Hash collisions are possible but in practice are often inconsequential.\n",
    "* Because the transformer is stateless, there is no mapping between word to feature index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<858x1048576 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9729 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "hashing_bag_of_words = HashingVectorizer(norm=None) # by default, it normalizes the vectors\n",
    "hashing_bag_of_words.fit(documents)\n",
    "hashing_bag_of_words.transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the feature matrix has over a million columns? This is in contrast from the result of the count vectorizer. The discrepancy is from the `HashingVectorizer` using, by default, $2^{20}=1048576$ different hash values to construct the count matrix. A vast majority of those indices will have no counts across all documents, and since we represent our feature matrix using a sparse matrix, we pay no cost for empty features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting time for CountVectorizer: 0.015514612197875977\n",
      "Fitting time for HashingVectorizer: 0.011745452880859375\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t_0 = time.time()\n",
    "CountVectorizer().fit_transform(documents)\n",
    "t_elapsed = time.time() - t_0\n",
    "print(\"Fitting time for CountVectorizer: {}\".format(t_elapsed))\n",
    "\n",
    "t_0 = time.time()\n",
    "HashingVectorizer(norm=None).fit_transform(documents)\n",
    "t_elapsed = time.time() - t_0\n",
    "print(\"Fitting time for HashingVectorizer: {}\".format(t_elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency-inverse document frequency\n",
    "\n",
    "Both the `CountVectorizer` and `HashingVectorizer` creates a feature matrix of raw counts. Using raw counts has two problems, documents vary widely in length and the counts will be large for common words such as \"the\" and \"is\". We need to use a weighting scheme that considers the aforementioned attributes. The term frequency-inverse document frequency, **tf-idf** for short, is a popular weighting scheme to improve the simple count based data from the bag of words model. It is the product of two values, the term frequency and the inverse document frequency. There are several variants but the most popular is defined below.\n",
    "\n",
    "* **Term Frequency:**\n",
    "$$ \\mathrm{tf}(t, d) = \\frac{\\mathrm{counts}(t, d)}{\\sqrt{\\sum_{t \\in d} \\mathrm{counts}(t, d)^2}}, $$\n",
    "    where $\\mathrm{counts}(t, d)$ is the raw count of term $t$ in document $d$ and $t \\in d$ are the terms in document $d$. The normalization results in a vector of unit length.\n",
    "\n",
    "* **Inverse Document Frequency:**\n",
    "$$ \\mathrm{idf}(t, D) = \\ln\\left(\\frac{\\text{number of documents in corpus } D}{1 + \\text{number of documents with term } t}\\right). $$\n",
    "    Every counted term $t$ in the corpus will have its own idf weight. The $1+$ in the denominator is to ensure no division by zero if a term does not appear in the corpus. The idf weight is simply the log of the inverse of a term's document frequency.\n",
    "    \n",
    "With both $\\mathrm{tf}(t, d)$ and $\\mathrm{idf}(t, D)$ calculated, the tf-idf weight is\n",
    "\n",
    "$$ \\mathrm{tfidf}(t, d, D) = \\mathrm{tf}(t, d) \\mathrm{idf}(t, D).$$\n",
    "\n",
    "With the idf weighting, words that are very common throughout the documents get weighted down. The reverse is true; the count of rare words get weighted up. With the tf-idf weighting scheme, a machine learning model will have an easier time to learn patterns to properly predict labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to apply the tf-idf weighting in `scikit-learn`, differing in what input they work on. `TfidfVectorizer` works on an array of documents (e.g., list of sentences) while the `TfidfTransformer` works on a count matrix, like the outputs of `HashingVectorizer` and `CountVectorizer`. `TfidfVectorizer` encapsulates the `CountVectorizer` and `TfidfTransformer` into one class. Since we have already calculated the word counts, we will demonstrate the `TfidfTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<858x2934 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9729 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2686)\t0.13457573249783877\n",
      "  (0, 2648)\t0.11488498883241917\n",
      "  (0, 2472)\t0.32089853352203146\n",
      "  (0, 2460)\t0.34746914252148997\n",
      "  (0, 2459)\t0.360084066500344\n",
      "  (0, 2230)\t0.3284474335768528\n",
      "  (0, 2228)\t0.23586886581754885\n",
      "  (0, 2096)\t0.24603505330410555\n",
      "  (0, 1759)\t0.32089853352203146\n",
      "  (0, 1595)\t0.34746914252148997\n",
      "  (0, 1407)\t0.152252591082911\n",
      "  (0, 286)\t0.360084066500344\n",
      "  (0, 235)\t0.12133613062597995\n",
      "  (1, 2899)\t0.2649613173291231\n",
      "  (1, 2873)\t0.2581129098076805\n",
      "  (1, 2648)\t0.18056648175221662\n",
      "  (1, 2434)\t0.18067851826439843\n",
      "  (1, 2153)\t0.2730612642269768\n",
      "  (1, 1850)\t0.16573016384510214\n",
      "  (1, 1837)\t0.11007402696888108\n",
      "  (1, 1563)\t0.2730612642269768\n",
      "  (1, 1542)\t0.29575556598331587\n",
      "  (1, 1541)\t0.23416706867493028\n",
      "  (1, 1507)\t0.2581129098076805\n",
      "  (1, 1427)\t0.29575556598331587\n",
      "  :\t:\n",
      "  (846, 820)\t0.771110084768154\n",
      "  (847, 261)\t1.0\n",
      "  (848, 1408)\t0.44921714868644663\n",
      "  (848, 138)\t0.44921714868644663\n",
      "  (848, 97)\t0.5460805373990351\n",
      "  (848, 69)\t0.5460805373990351\n",
      "  (849, 2574)\t0.6409649776877993\n",
      "  (849, 1620)\t0.5578074350621605\n",
      "  (849, 53)\t0.5272710524645294\n",
      "  (850, 2096)\t0.20265981408443645\n",
      "  (850, 2051)\t0.4087091737342693\n",
      "  (850, 1312)\t0.22521572053168357\n",
      "  (850, 870)\t0.555442707401837\n",
      "  (850, 76)\t0.6577591421487233\n",
      "  (851, 2854)\t0.5773502691896258\n",
      "  (851, 2045)\t0.5773502691896258\n",
      "  (851, 172)\t0.5773502691896258\n",
      "  (852, 1408)\t0.7071067811865475\n",
      "  (852, 138)\t0.7071067811865475\n",
      "  (853, 121)\t0.7071067811865475\n",
      "  (853, 82)\t0.7071067811865475\n",
      "  (856, 1538)\t0.7195993849879749\n",
      "  (856, 1002)\t0.6943894621355714\n",
      "  (857, 2842)\t0.7426080690748966\n",
      "  (857, 1842)\t0.669726254334451\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_weights = tfidf.fit_transform(word_counts)\n",
    "print(tfidf_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We no longer have raw counts in our feature matrix. Let's use the `idf_` attribute of the fitted tf-idf transformer to inspect the top idf weights and their corresponding terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.06262174142431 zope\n",
      "7.06262174142431 grown\n",
      "7.06262174142431 gutted\n",
      "7.06262174142431 guinea\n",
      "7.06262174142431 guide\n",
      "7.06262174142431 guard\n",
      "7.06262174142431 grumpy\n",
      "7.06262174142431 grows\n",
      "7.06262174142431 growing\n",
      "7.06262174142431 hello\n",
      "7.06262174142431 groovy\n",
      "7.06262174142431 gripping\n",
      "7.06262174142431 gripped\n",
      "7.06262174142431 greatest\n",
      "7.06262174142431 greater\n",
      "7.06262174142431 graphviz\n",
      "7.06262174142431 göttingen\n",
      "7.06262174142431 habitation\n",
      "7.06262174142431 hall\n",
      "7.06262174142431 hamilton\n",
      "7.06262174142431 handle\n",
      "7.06262174142431 handled\n",
      "7.06262174142431 handling\n",
      "7.06262174142431 happened\n",
      "7.06262174142431 hardware\n",
      "7.06262174142431 haskell\n",
      "7.06262174142431 hat\n",
      "7.06262174142431 hatchlings\n",
      "7.06262174142431 haydnmacphiei\n"
     ]
    }
   ],
   "source": [
    "top_idf_indices = tfidf.idf_.argsort()[:-30:-1]\n",
    "ind_to_word = bag_of_words.get_feature_names()\n",
    "\n",
    "for ind in top_idf_indices:\n",
    "    print(tfidf.idf_[ind], ind_to_word[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using tf-idf weighting renders the process as _stateful_; to apply the idf weight, we need to know the frequency of each word across all documents. While we may initially use `HasingVectorizer` to have a stateless transformer, coupling it with `TfidfTransformer` will create a stateful process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving signal\n",
    "\n",
    "So far, we have discussed how using tf-idf rather than raw counts will improve the performance of our machine learning model. There are several other approaches that can boost performance; we will discuss techniques that improve the signal in our data set. Note, the following techniques may marginally increase model performance. It may be best to create a baseline model and measure the increased performance with the new model additions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "\n",
    "Words such as \"the\", \"a\", and \"or\" are so common throughout our corpus that they do not contribute any signal to our data set. Further, omitting these words will reduce our already high dimensional data set. It is best to not have these words as features and not be counted in the analysis. The set of words that will not factor into our analysis are called **stop words**.\n",
    "\n",
    "spaCy provides a `set` of around 300 commonly used English words. When using stop words, it is best to examine the entries in case there are certain words you want to be included or not included. Since the words are provided as a Python `set`, we can use methods available to `set` objects to modify entries of the `set` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'set'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'could',\n",
       " 'did',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'indeed',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'made',\n",
       " 'make',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " \"n't\",\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'n‘t',\n",
       " 'n’t',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'python',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 're',\n",
       " 'really',\n",
       " 'regarding',\n",
       " 'same',\n",
       " 'say',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'under',\n",
       " 'unless',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'used',\n",
       " 'using',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " '‘d',\n",
       " '‘ll',\n",
       " '‘m',\n",
       " '‘re',\n",
       " '‘s',\n",
       " '‘ve',\n",
       " '’d',\n",
       " '’ll',\n",
       " '’m',\n",
       " '’re',\n",
       " '’s',\n",
       " '’ve'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import STOP_WORDS\n",
    "\n",
    "print(type(STOP_WORDS))\n",
    "STOP_WORDS_python = STOP_WORDS.union({\"python\"})\n",
    "STOP_WORDS_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'python' in STOP_WORDS_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using stop words with `scikit-learn` the set of tokens needs to be in the form they would be in if they had gone through the same [preprocessing and tokenization as the vectorizer](https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words). For example, the set of stop words provided by SpaCy includes several contractions, e.g., `\"'d\"` and `\"'ll\"`. Notice the apostrophe; the apostrophe in words will be removed once they are processed by the vectorizer. As such, we need to make sure words do not have apostrophes in the set of stop words. If we include the apostrophe, `scikit-learn` will be nice enough to raise a warning letting us know about the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = {stop_word.replace(\"'\", \"\") for stop_word in STOP_WORDS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmatization\n",
    "\n",
    "In our current analysis, words like \"python\" and \"pythons\" will be counted as separate words. We understand that they represent the same concept and want them to be treated as the same word. The same applies to other words like \"run\", \"runs\", \"ran\", and \"running\", they all represent the same meaning. **Stemming** is the process of reducing a word to its stem. Note, the stemming process is not 100% effective and sometimes the resulting stem is not an actual word. For example, the popular Porter stemming algorithm applied to \"argues\" and \"arguing\" returns `\"argu\"`.\n",
    "\n",
    "**Lemmatization** is the process of reducing a word to its lemma, or the dictionary form of the word. It is a more sophisticated process than stemming as it considers context and part of speech. Further, the resulting lemma is an actual word. spaCy does not have a stemming algorithm but does offer lemmatization. Each word analyzed by spaCy has the attribute `lemma_` which returns the lemma of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'run', 'run', 'run']\n",
      "['buy', 'buy', 'buy', 'buy']\n",
      "['see', 'see', 'see', 'see']\n"
     ]
    }
   ],
   "source": [
    "print([word.lemma_ for word in nlp('run runs ran running')])\n",
    "print([word.lemma_ for word in nlp('buy buys buying bought')])\n",
    "print([word.lemma_ for word in nlp('see saw seen seeing')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply lemmatization in `scikit-learn`, you need to pass a function to the keyword `tokenizer` of whatever text vectorizer you are deploying. See the example below were we apply lemmatization for a `TfidfVectorizer` transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def lemmatizer(text):\n",
    "    return [word.lemma_.lower() for word in nlp(text)]\n",
    "\n",
    "# we need to generate the lemmas of the stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'be', 'run', 'quickly', '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(\"Dogs are running quickly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '\\n\\n', '\\n\\n\\n', ' ', '\"', '(', ')', '+', ',', '-', '.', '0', '1', '2', '3', '4', ':', ';', '<', '=', 'add', 'allow', 'b', 'ball', 'block', 'c', 'captivity', 'class', 'code', 'common', 'compile', 'consider', 'cpython', 'describe', 'design', 'development', 'division', 'e.g.', 'eat', 'egg', 'enclosure', 'example', 'expression', 'feature', 'female', 'find', 'ft', 'function', 'help', 'human', 'implementation', 'include', 'integer', 'island', 'java', 'kill', 'language', 'large', 'length', 'library', 'like', 'list', 'long', 'm.', 'male', 'method', 'module', 'new', 'number', 'object', 'old', 'operator', 'oz', 'pattern', 'pet', 'prey', 'program', 'programming', 'propose', 'provide', 'r.', 'range', 'reference', 'release', 'report', 'reticulate', 'small', 'snake', 'standard', 'statement', 'string', 'support', 'syntax', 'time', 'type', 'value', 'variable', 'version', 'write', 'year']\n"
     ]
    }
   ],
   "source": [
    "stop_words_str = \" \".join(STOP_WORDS) # nlp function needs a string\n",
    "stop_words_lemma = set(word.lemma_.lower() for word in nlp(stop_words_str))\n",
    "\n",
    "tfidf_lemma = TfidfVectorizer(max_features=100,\n",
    "                              stop_words=stop_words_lemma.union({\"python\"}),\n",
    "                              tokenizer=lemmatizer)\n",
    "\n",
    "tfidf_lemma.fit(documents)\n",
    "print(tfidf_lemma.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and n-grams\n",
    "\n",
    "Tokenization refers to dividing up a document into pieces to be counted. In our analysis so far, we are only counting words. However, it may be useful to count a sequence of words such as \"natural environment\" and \"virtual environment\". Counting these **bigrams** for our word usage analyzer may boost performance. More generally, an n-gram refers to the n sequence of words. In `scikit-learn`, n-grams can be included by setting `ngram_range=(min_n, max_n)` for the vectorizer, where `min_n` and `max_n` are the lower and upper bound of the range of n-grams to include. For example, `ngram_range=(1, 2)` will include words and bigrams while `ngram_range=(2, 2)` will only count bigrams. Let's see what are the most frequent bigrams in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['23 ft',\n",
       " 'ball care',\n",
       " 'ball pythons',\n",
       " 'design philosophy',\n",
       " 'floating point',\n",
       " 'ft length',\n",
       " 'guido van',\n",
       " 'isbn 978',\n",
       " 'lb oz',\n",
       " 'new features',\n",
       " 'object oriented',\n",
       " 'oriented programming',\n",
       " 'programming language',\n",
       " 'programming languages',\n",
       " 'reticulated pythons',\n",
       " 'scripting language',\n",
       " 'spam eggs',\n",
       " 'standard library',\n",
       " 'van rossum',\n",
       " 'year old']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_counter = CountVectorizer(max_features=20, ngram_range=(2, 2), stop_words=STOP_WORDS.union({\"python\"}))\n",
    "bigram_counter.fit(documents)\n",
    "\n",
    "bigram_counter.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "* Is using stop words more important when using `CountVectorizer`/`HashingVectorizer` or when using the `TfidfVectorizer`?\n",
    "* Is it practical to use a large n-gram range, for example, count 3-grams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "\n",
    "* _As TfidfVectorizer has an idf element which under weighs the most common words,i.e the stop words, it would be more important to include in the CountVectorizer/HashingVectorizer_\n",
    "\n",
    "* _If we use a large n_gram range, we are naturally increasing the number of features which could potentially lead to overfitting._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document similarity\n",
    "\n",
    "After we have transformed our corpus into a matrix, we can interpret our data set as representing a set of vectors in a $p$-dimensional space, where each document is its own vector. One common analysis is to find similar documents. The cosine similarity is a metric that measure how well aligned in space are two vectors, equal to the cosine of the angle in between the two vectors. If the vectors are perfectly aligned, they point in the same direction, the angle they form is 0 and the similarity score is 1. If the vectors are orthogonal, forming an angle of 90 degrees, the similarity metric is 0. Mathematically, the cosine similarity metric is equal to the dot product of two vectors, normalized,\n",
    "\n",
    "$$ \\frac{v_1 \\cdot v_2}{\\|v_1 \\|\\|v_2 \\|}, $$\n",
    "\n",
    "where $v_1$ and $v_2$ are two document vectors and $\\| v_1 \\|$ and $\\| v_2 \\|$ are their lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word usage classifier\n",
    "\n",
    "Let's build a word usage classifier with all the techniques we have seen. The model will include:\n",
    "\n",
    "* tf-idf weighting\n",
    "* stop words\n",
    "* words and bigrams\n",
    "* lemmatization\n",
    "\n",
    "Applying the above techniques should result in a data set with enough signal that a machine learning model can learn from. For this exercise, we will use the naive Bayes model; a probabilistic model that calculates conditional probabilities using Bayes theorem. The term naive is applied because it assumes the features are conditionally independent from each other. You can think of a naive Bayes classifier working by determining what class should a document be assigned based upon the frequencies of words in the different classes in the training set. Naive Bayes is often used as benchmark model for NLP as it is quick to train. More about the model in general can be found [here](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) and details of the `scikit-learn` implementation is found [here](https://scikit-learn.org/stable/modules/naive_bayes.html). After training our model, we will see how well it performs for a chosen set of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9242424242424242\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# create data set and labels\n",
    "documents = animal_sents + language_sents\n",
    "labels = [\"animal\"]*len(animal_sents) + [\"language\"]*len(language_sents)\n",
    "\n",
    "# lemma of stop words\n",
    "stop_words_str = \" \".join(STOP_WORDS)\n",
    "stop_words_lemma = set(word.lemma_.lower() for word in nlp(stop_words_str))\n",
    "\n",
    "# create and train pipeline\n",
    "tfidf = TfidfVectorizer(stop_words=stop_words_lemma, tokenizer=lemmatizer, ngram_range=(1, 2))\n",
    "pipe = Pipeline([('vectorizer', tfidf), ('classifier', MultinomialNB())])\n",
    "pipe.fit(documents, labels)\n",
    "\n",
    "print(\"Training accuracy: {}\".format(pipe.score(documents, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Python program is only 100 bytes long. --> language at 59.2885%\n",
      "A python's bite is not venomous but still hurts. --> animal at 55.8632%\n",
      "I can't find the error in the python code. --> language at 78.0464%\n",
      "Where is my pet python; I can't find her! --> animal at 57.3965%\n",
      "I use for and while loops when writing Python. --> language at 82.0856%\n",
      "The python will loop and wrap itself onto me. --> language at 64.7964%\n",
      "I use snake case for naming my variables. --> language at 55.1759%\n",
      "My python has grown to over 10 ft long! --> animal at 66.8943%\n",
      "I use virtual environments to manage package versions. --> language at 74.9604%\n",
      "Pythons are the largest snakes in the environment. --> animal at 74.3139%\n"
     ]
    }
   ],
   "source": [
    "test_docs = [\"My Python program is only 100 bytes long.\",\n",
    "             \"A python's bite is not venomous but still hurts.\",\n",
    "             \"I can't find the error in the python code.\",\n",
    "             \"Where is my pet python; I can't find her!\",\n",
    "             \"I use for and while loops when writing Python.\",\n",
    "             \"The python will loop and wrap itself onto me.\",\n",
    "             \"I use snake case for naming my variables.\",\n",
    "             \"My python has grown to over 10 ft long!\",\n",
    "             \"I use virtual environments to manage package versions.\",\n",
    "             \"Pythons are the largest snakes in the environment.\"]\n",
    "\n",
    "class_labels = [\"animal\", \"language\"]\n",
    "y_proba = pipe.predict_proba(test_docs)\n",
    "predicted_indices = (y_proba[:, 1] > 0.5).astype(int)\n",
    "\n",
    "for i, index in enumerate(predicted_indices):\n",
    "    print(test_docs[i], \"--> {} at {:g}%\".format(class_labels[index], 100*y_proba[i, index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Encapsulate the entire process of gathering a corpus, constructing, and training a model into a function. Afterwards, deploy the model to other sets of homonyms.\n",
    "1. Measure the model's improvements by stripping out things such as the use of stop words and lemmatization. Perhaps you can incorporate model additions as parameters to the previously mentioned function. What model additions increases the performance the most?\n",
    "1. Consider another source of data and see how well the model performs with the new corpus.\n",
    "1. Naive Bayes classifier calculates conditional probabilities from the training set. In other words, it determines values like $P(\\text{snake | }Y = \\text{animal})$, the probability a document has the word \"snake\" given if the document belongs to those of python the animal. These values are stored in `coef_` attribute of a trained naive Bayes model. Can you use these coefficients to determine the most discriminative features? In other words, what terms when found in a document really help classify the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Exercise solution-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def pages_to_sentences(*pages):\n",
    "    \"\"\"Return a list of sentences in Wikipedia articles.\"\"\"\n",
    "    sentences = []\n",
    "    \n",
    "    for page in pages:\n",
    "        p = wikipedia.page(page)\n",
    "        doc = nlp(p.content)\n",
    "        sentences += [sent.text for sent in doc.sents]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_corpus(words):\n",
    "    corpus=[]\n",
    "    labels=[]\n",
    "    \n",
    "    for k, v in words.items():\n",
    "        docs=pages_to_sentences(*v)\n",
    "        corpus+=docs\n",
    "        labels+=[k]* len(docs)\n",
    "    return corpus,labels\n",
    "\n",
    "\n",
    "words={'greek': ['Amazons'],\n",
    "      'rainforest': ['Amazon_rainforest'],\n",
    "      'company': ['Amazon_(company)']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpus,labels=get_corpus(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1251\n",
      "['In Greek mythology, the Amazons (Ancient Greek:', 'Ἀμαζόνες Amazónes, singular Ἀμαζών Amazōn) are portrayed in a number of ancient epic poems and legends, such as the Labours of Hercules, the Argonautica  and the Iliad.', 'They were a people of female warriors and hunters, who matched men in physical agility and strength, in archery, riding skills and the arts of combat.', 'Their society was closed for men and they only raised their daughters, either killing their sons or returning them to their fathers, with whom they would only socialize briefly in order to reproduce.', 'Courageous and fiercely independent, the Amazons, commanded by their queen, regularly undertook extensive military expeditions into the far corners of the world, from Scythia to Thrace, Asia Minor and the Aegean Islands, reaching as far as Arabia and Egypt.']\n",
      "['greek', 'greek', 'greek', 'greek', 'greek']\n",
      "['=', 'External links ==\\n', 'Official website \\n', 'Amazon (company) companies grouped at OpenCorporates\\nBusiness data for Amazon.com,', 'Inc.:']\n",
      "['company', 'company', 'company', 'company', 'company']\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))\n",
    "print(corpus[:5])\n",
    "print(labels[:5])\n",
    "print(corpus[-5:])\n",
    "print(labels[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise solution - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1,1) or (1,2)\n",
    "#lemmatize or dont lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_sents = pages_to_sentences(\"Reticulated python\", \"Ball Python\")\n",
    "language_sents = pages_to_sentences(\"Python (programming language)\")\n",
    "documents = animal_sents + language_sents\n",
    "labels = [\"animal\"]*len(animal_sents) + [\"language\"]*len(language_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(documents,labels,test_size=0.2,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
       "                                       ('clf', MultinomialNB())]),\n",
       "             param_grid={'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'vectorizer__tokenizer': [None,\n",
       "                                                   <function lemmatizer at 0x7f37c6ecdca0>]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe=Pipeline([('vectorizer',TfidfVectorizer()),('clf',MultinomialNB())])\n",
    "\n",
    "param_grid={'vectorizer__ngram_range':[(1,1),(1,2)],\n",
    "            'vectorizer__tokenizer': [None,lemmatizer]}\n",
    "\n",
    "grid_search=GridSearchCV(pipe,param_grid,cv=5,verbose=1)\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01403027, 6.15527568, 0.02939582, 5.94923038]),\n",
       " 'std_fit_time': array([0.00074606, 0.1805483 , 0.00139402, 0.17808508]),\n",
       " 'mean_score_time': array([0.0031354 , 1.49132605, 0.00464001, 1.46687884]),\n",
       " 'std_score_time': array([0.00021516, 0.06706788, 0.00033331, 0.04743409]),\n",
       " 'param_vectorizer__ngram_range': masked_array(data=[(1, 1), (1, 1), (1, 2), (1, 2)],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vectorizer__tokenizer': masked_array(data=[None, <function lemmatizer at 0x7f37c6ecdca0>, None,\n",
       "                    <function lemmatizer at 0x7f37c6ecdca0>],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'vectorizer__ngram_range': (1, 1), 'vectorizer__tokenizer': None},\n",
       "  {'vectorizer__ngram_range': (1, 1),\n",
       "   'vectorizer__tokenizer': <function __main__.lemmatizer(text)>},\n",
       "  {'vectorizer__ngram_range': (1, 2), 'vectorizer__tokenizer': None},\n",
       "  {'vectorizer__ngram_range': (1, 2),\n",
       "   'vectorizer__tokenizer': <function __main__.lemmatizer(text)>}],\n",
       " 'split0_test_score': array([0.86956522, 0.87681159, 0.86231884, 0.84057971]),\n",
       " 'split1_test_score': array([0.85507246, 0.86231884, 0.82608696, 0.82608696]),\n",
       " 'split2_test_score': array([0.86131387, 0.86131387, 0.88321168, 0.86861314]),\n",
       " 'split3_test_score': array([0.86131387, 0.83211679, 0.81751825, 0.82481752]),\n",
       " 'split4_test_score': array([0.84671533, 0.81021898, 0.82481752, 0.80291971]),\n",
       " 'mean_test_score': array([0.85879615, 0.84855601, 0.84279065, 0.83260341]),\n",
       " 'std_test_score': array([0.00759556, 0.02404385, 0.02551843, 0.02165423]),\n",
       " 'rank_test_score': array([1, 2, 3, 4], dtype=int32)}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'vectorizer__ngram_range': (1, 1), 'vectorizer__tokenizer': None},\n",
       " {'vectorizer__ngram_range': (1, 1),\n",
       "  'vectorizer__tokenizer': <function __main__.lemmatizer(text)>},\n",
       " {'vectorizer__ngram_range': (1, 2), 'vectorizer__tokenizer': None},\n",
       " {'vectorizer__ngram_range': (1, 2),\n",
       "  'vectorizer__tokenizer': <function __main__.lemmatizer(text)>}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85879615, 0.84855601, 0.84279065, 0.83260341])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectorizer__ngram_range': (1, 1), 'vectorizer__tokenizer': None}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8587961493705702"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2021 WorldQuant University. This content is licensed solely for personal use. Redistribution or publication of this material is strictly prohibited.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
